import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from model import ChatbotModel
from dataset_loader import load_dataset
import json
import os
import time
import warnings

warnings.filterwarnings("ignore", category=RuntimeWarning)

def main():
    with open("config.json", "r", encoding="utf-8") as f:
        config = json.load(f)

    VOCAB_SIZE = config["vocab_size"]
    EMBED_DIM = config["embed_dim"]
    HIDDEN_DIM = config["hidden_dim"]
    NUM_LAYERS = config["num_layers"]
    BATCH_SIZE = config["batch_size"]
    NUM_WORKERS = config["num_workers"]
    CHECKPOINT_PATH = config["checkpoint_path"]

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset, vocab = load_dataset("dataset.csv", VOCAB_SIZE)
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(vocab)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤.")

    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
    print(f"‚úÖ DataLoader —Å–æ–∑–¥–∞–Ω! {len(train_loader)} –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")

    if os.path.exists(CHECKPOINT_PATH):
        print(f"üîÑ –ù–∞–π–¥–µ–Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ {CHECKPOINT_PATH}, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
        model = ChatbotModel.from_pretrained(checkpoint["model_state"], new_vocab_size=VOCAB_SIZE)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        start_epoch = checkpoint["epoch"]
        start_batch = checkpoint["batch"]
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: –≠–ø–æ—Ö–∞ {start_epoch}, –ë–∞—Ç—á {start_batch}")
    else:
        print("‚ö†Ô∏è –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
        model = ChatbotModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        start_epoch = 0
        start_batch = 0

    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞", next(model.parameters()).device)

    for epoch in range(start_epoch, config["epochs"]):
        epoch_loss = 0
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            if epoch == start_epoch and batch_idx < start_batch:
                continue

            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = F.cross_entropy(outputs.view(-1, VOCAB_SIZE), targets.view(-1))
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            if batch_idx % 10 == 0:
                print(f"üü¢ –≠–ø–æ—Ö–∞ {epoch+1}/{config['epochs']}, –ë–∞—Ç—á {batch_idx}/{len(train_loader)}, –ü–æ—Ç–µ—Ä–∏: {loss.item():.4f}")

            if batch_idx % 50 == 0:
                torch.save({
                    "epoch": epoch,
                    "batch": batch_idx,
                    "model_state": model.state_dict(),
                    "optimizer_state": optimizer.state_dict(),
                }, CHECKPOINT_PATH)
                print(f"üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω! (—ç–ø–æ—Ö–∞ {epoch+1}, –±–∞—Ç—á {batch_idx})")

        print(f"‚úÖ –≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏: {epoch_loss / len(train_loader):.4f}")

    torch.save(model.state_dict(), "trained_model/chatbot.pth")
    print("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    os.remove(CHECKPOINT_PATH)

if __name__ == "__main__":
    main()