import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from model import ChatbotModel
from dataset_loader import load_dataset
import json
import os
import warnings

warnings.filterwarnings("ignore", category=RuntimeWarning)

with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

VOCAB_SIZE = config["vocab_size"]
EMBED_DIM = config["embed_dim"]
HIDDEN_DIM = config["hidden_dim"]
NUM_LAYERS = config["num_layers"]
EPOCHS = config["epochs"]
BATCH_SIZE = config["batch_size"]
NUM_WORKERS = config["num_workers"]
CHECKPOINT_PATH = config["checkpoint_path"]

def create_optimizer(model, lr=0.001):
    return optim.Adam(model.parameters(), lr=lr)

if __name__ == "__main__":
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset, vocab = load_dataset("dataset.csv", VOCAB_SIZE)
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(vocab)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤.")

    os.makedirs("trained_model", exist_ok=True)
    with open("trained_model/vocab.json", "w", encoding="utf-8") as f:
        json.dump(vocab, f, ensure_ascii=False, indent=4)
    print("‚úÖ –í–æ–∫–∞–±—É–ª—è—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω!")

    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
    print(f"‚úÖ DataLoader —Å–æ–∑–¥–∞–Ω! {len(train_loader)} –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")

    start_epoch = 0
    start_batch = 0
    if os.path.exists(CHECKPOINT_PATH):
        print(f"üîÑ –ù–∞–π–¥–µ–Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ {CHECKPOINT_PATH}, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device('cpu'))
        old_vocab_size = checkpoint["model_state"]["embedding.weight"].shape[0]
        
        if old_vocab_size < VOCAB_SIZE:
            print(f"üîÑ –†–∞—Å—à–∏—Ä—è–µ–º –º–æ–¥–µ–ª—å —Å {old_vocab_size} –¥–æ {VOCAB_SIZE} —Å–ª–æ–≤...")
            model = ChatbotModel.from_pretrained(checkpoint["model_state"], new_vocab_size=VOCAB_SIZE)
            optimizer = create_optimizer(model)
        else:
            model = ChatbotModel.from_pretrained(checkpoint["model_state"])
            optimizer = create_optimizer(model)
            optimizer.load_state_dict(checkpoint["optimizer_state"])
        
        start_epoch = checkpoint["epoch"]
        start_batch = checkpoint["batch"]
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è —Å —ç–ø–æ—Ö–∏ {start_epoch + 1}, –±–∞—Ç—á–∞ {start_batch}.")
    else:
        print("‚ö†Ô∏è –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
        model = ChatbotModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS)
        optimizer = create_optimizer(model)

    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    for epoch in range(start_epoch, EPOCHS):
        epoch_loss = 0
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            if batch_idx < start_batch:
                continue

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = F.cross_entropy(outputs.view(-1, VOCAB_SIZE), targets.view(-1))
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            if batch_idx % 10 == 0:
                print(f"üü¢ –≠–ø–æ—Ö–∞ {epoch+1}/{EPOCHS}, –ë–∞—Ç—á {batch_idx}/{len(train_loader)}, –ü–æ—Ç–µ—Ä–∏: {loss.item():.4f}")

            if batch_idx % 50 == 0:
                torch.save({
                    "epoch": epoch,
                    "batch": batch_idx,
                    "model_state": model.state_dict(),
                    "optimizer_state": optimizer.state_dict(),
                }, CHECKPOINT_PATH)
                print(f"üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω! (—ç–ø–æ—Ö–∞ {epoch+1}, –±–∞—Ç—á {batch_idx})")

        print(f"‚úÖ –≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏: {epoch_loss / len(train_loader):.4f}")

    torch.save(model.state_dict(), "trained_model/chatbot.pth")
    print("‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    os.remove(CHECKPOINT_PATH)